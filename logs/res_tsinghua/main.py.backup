from __future__ import print_function
import os
import argparse
import sklearn.metrics as metrics
import numpy as np
import torch
from torch.nn import functional as F
from tqdm import tqdm
from matplotlib import pyplot as plt
from torchvision import transforms
from torch.utils.data import DataLoader
from sklearn.svm import SVC
from sklearn.metrics import precision_recall_curve, auc,average_precision_score, precision_score, recall_score
from data import load_and_split_data, load_Tsinghua_data, StairDataset
from model import ResNet18
from utils import IOStream

def _init_(args):
    if not os.path.exists('logs'):
        os.makedirs('logs')
    if not os.path.exists('logs/'+args.exp_name):
        os.makedirs('logs/'+args.exp_name)
    if not os.path.exists('logs/'+args.exp_name+'/'+'models'):
        os.makedirs('logs/'+args.exp_name+'/'+'models')
    os.system('cp main.py logs'+'/'+args.exp_name+'/'+'main.py.backup')
    os.system('cp model.py logs' + '/' + args.exp_name + '/' + 'model.py.backup')
    os.system('cp data.py logs' + '/' + args.exp_name + '/' + 'data.py.backup')


def evaluate(io, test_label, test_prob, test_pred):
    ap = average_precision_score(test_label, test_prob)
    io.cprint(f"mAP(AP): {ap:.4f}")
    
    # PRC曲线
    precision, recall, thresholds = precision_recall_curve(test_label, test_prob)
    prc_auc = auc(recall, precision)
    plt.figure()
    plt.plot(recall, precision, 'm-', label='AUC = %.4f' % (prc_auc))
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower right")
    plt.savefig("Precision-Recall.png", dpi=300)
    
    # 直接给出类别（阈值为0.5）方法的precision和recall
    score_precision = precision_score(test_label, test_pred)
    score_recall = recall_score(test_label, test_pred)
    io.cprint("Directly calculate precision and recall:")
    io.cprint(f"Precision: {score_precision}")
    io.cprint(f"Recall: {score_recall}")
    
    # 根据曲线确定阈值后计算precision和recall
    plt.figure()
    plt.plot(thresholds, precision[:-1], c ='r', label ='PRECISION')
    plt.plot(thresholds, recall[:-1], c ='b', label ='RECALL')
    plt.xlabel('threshold')
    plt.ylabel('Recall/Precision')
    plt.xticks(np.arange(0, 1.0, step=0.1))
    plt.grid()
    plt.legend()
    plt.title('Precision-Recall-threshold')
    plt.savefig("Precision-Recall-threshold.png", dpi=300)
    
    ## 观察曲线修改 ##
    decision_threshold = 0.12
    ################
    
    adjested_pred = []
    for i in test_prob:
        if i < decision_threshold:
            adjested_pred.append(0)
        else:
            adjested_pred.append(1)
    score_precision = precision_score(test_label, np.array(adjested_pred))
    score_recall = recall_score(test_label, np.array(adjested_pred))
    io.cprint(f"Set threshold to {decision_threshold} and calculate precision and recall:")
    io.cprint(f"Precision: {score_precision}")
    io.cprint(f"Recall: {score_recall}")

  
def train(args, io, model, train_loader, valid_loader, optimizer, scheduler, device, epochs):
    model.to(device)
    
    train_losses = torch.ones(epochs)
    train_accs = torch.ones(epochs)
    valid_losses = torch.ones(epochs)
    valid_accs = torch.ones(epochs)

    best_acc = 0.0
    cnt = 0
    for epoch in range(epochs):
        # train
        model.train()
        io.cprint("\nEpoch %d | Learning Rate is %.6f" % (epoch+1, optimizer.param_groups[0]['lr']))
        count = 0
        train_loss = 0.0
        train_acc = 0.0
        train_pred = []
        train_true = []
        for data, label in tqdm(train_loader):
            data = data.float().to(device)
            label = label.float().to(device)
            batch_size = data.size()[0]
            
            optimizer.zero_grad()
            
            y_pred = model(data)
            # print(y_pred.shape)
            # print(label.shape)
            l = F.cross_entropy(y_pred, label.long())
            l.backward()
            
            optimizer.step()
            y_pred = F.softmax(y_pred, dim=-1)
            preds = y_pred.max(dim=1)[1]
            count += batch_size
            train_loss += l.item() * batch_size
            train_true.append(label.cpu().numpy())
            train_pred.append(preds.detach().cpu().numpy())
        
        train_true = np.concatenate(train_true)
        train_pred = np.concatenate(train_pred)
        
        
        train_acc = metrics.accuracy_score(train_true, train_pred)
        train_loss = train_loss*1.0/count
        train_losses[epoch] = train_loss
        train_accs[epoch] = train_acc
        
        io.cprint("Epoch %d | Mean train loss is %.6f, Train acc is %.3f" % 
              (epoch+1, train_loss, train_acc*100.00))  

        scheduler.step()
        
        # valid
        model.eval()
        count = 0
        valid_loss = 0.0
        valid_acc = 0.0
        valid_true = []
        valid_pred = []
        for data, label in tqdm(valid_loader):
            data = data.float().to(device)
            label = label.float().to(device)
            batch_size = data.size()[0]
            
            y_pred = model(data)
            
            l = F.cross_entropy(y_pred, label.long())
            
            y_pred = F.softmax(y_pred, dim=-1)
            preds = y_pred.max(dim=1)[1]
            count += batch_size
            
            valid_loss += l.item() * batch_size
            valid_true.append(label.cpu().numpy())
            valid_pred.append(preds.detach().cpu().numpy())
                
        
        valid_true = np.concatenate(valid_true)
        valid_pred = np.concatenate(valid_pred)
        valid_acc = metrics.accuracy_score(valid_true, valid_pred)
        valid_loss = valid_loss*1.0/count
        
        valid_losses[epoch] = valid_loss
        valid_accs[epoch] = valid_acc
        
        if valid_acc >= best_acc:
            best_acc = valid_acc
            torch.save(model.state_dict(), './logs/%s/models/model.t7' % args.exp_name)
            cnt=0
        else:
            cnt+=1
        
        io.cprint("Epoch %d | Mean valid loss is %.6f, valid acc is %.3f" % 
                (epoch+1, valid_loss, valid_acc*100.00))
        io.cprint("Best valid acc is %.3f" % (best_acc*100.00))
        if cnt >= 6:
            train_accs = train_accs[:epoch]
            train_losses = train_losses[:epoch]
            valid_accs = valid_accs[:epoch]
            valid_losses = valid_losses[:epoch]
            break
        
        
    # 画图
    l = len(train_losses)
    plt.figure()
    plt.plot(torch.arange(0, l, step=1), train_accs, 'b', label='Training accuracy')
    plt.plot(torch.arange(0, l, step=1), valid_accs, 'r', label='validation accuracy')
    plt.title('Training and validation accuracy')
    plt.legend(loc='lower right')
    plt.savefig(("./logs/%s/acc.jpg" % args.exp_name), dpi=300)
    plt.figure()
    plt.plot(torch.arange(0, l, step=1), train_losses, 'b', label='Training loss')
    plt.plot(torch.arange(0, l, step=1), valid_losses, 'r', label='validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.savefig(("./logs/%s/loss.jpg" % args.exp_name), dpi=300)
    

def test_and_eval(args, io, model, test_loader, device, eval=True, best_path=None):
    model.to(device)
    if best_path is not None:
        model.load_state_dict(torch.load(best_path))
    else:
        model.load_state_dict(torch.load(('./logs/%s/models/model.t7' % args.exp_name)))
    io.cprint("Start to test...")
    model.eval()
    
    count = 0
    test_loss = 0.0
    test_acc = 0.0
    test_true = []
    test_prob = []
    test_pred = []
    with torch.no_grad():
        for data, label in tqdm(test_loader):
            data = data.float().to(device)
            label = label.float().to(device)
            batch_size = data.size()[0]
            
            y_pred = model(data)
            
            l = F.cross_entropy(y_pred, label.long())
            
            y_pred = F.softmax(y_pred, dim=-1)
            preds = y_pred.max(dim=1)[1]
            # 保存预测为1类的概率
            
            count += batch_size
            
            test_loss += l.item() * batch_size
            test_true.append(label.cpu().numpy())
            test_prob.append(y_pred[:, 1].detach().cpu().numpy())
            test_pred.append(preds.detach().cpu().numpy())
        
        test_true = np.concatenate(test_true)
        test_prob = np.concatenate(test_prob)
        test_pred = np.concatenate(test_pred)
        test_acc = metrics.accuracy_score(test_true, test_pred)
        test_loss = test_loss*1.0/count
        
        io.cprint("Mean test loss is %.6f, test acc is %.3f" % (test_loss, test_acc*100.00))
        if eval:
            evaluate(io, test_true, test_prob, test_pred)


def main(args, io, eval=True):
    if args.model == 'svm':
        model = SVC(kernel='rbf', probability=True)
    elif args.model == 'resnet':
        model = ResNet18(num_classes=2, use_dropout=False, use_init=False)
    io.cprint(str(model))
    
    # 加载数据
    io.cprint("Preparing data...")
    train_transform = transforms.Compose([
        # transforms.RandomRotation(30, center=(0, 0), expand=True),
        
        # transforms.RandomResizedCrop(size=256),
        
        # transforms.RandomHorizontalFlip(0.5),
        # transforms.RandomRotation(90),
        
        transforms.Resize([96, 96]),
        transforms.ToTensor(),
        # transforms.Normalize(mean, std)
    ]) 
    test_transform = transforms.Compose([
        # transforms.RandomHorizontalFlip(0.5),
        # transforms.RandomRotation(90),
        transforms.Resize([96, 96]),
        transforms.ToTensor(),
        # transforms.Normalize(mean, std)
    ])
    trainset, validset, testset = load_and_split_data('./stair/public/')
    train_set = StairDataset(trainset, 'train', train_transform)
    valid_set = StairDataset(validset, 'valid', test_transform)
    test_set = StairDataset(testset, 'test', test_transform)
    
    
    if isinstance(model, SVC):  
        train_data = []
        train_label = []
        test_data = []
        test_label = []
        train_loader = DataLoader(train_set, batch_size=1, num_workers=16, shuffle=True, drop_last=False)
        test_loader = DataLoader(test_set, batch_size=1, num_workers=16, shuffle=False, drop_last=False)
        for idx, (img, label) in enumerate(train_loader):
            train_data.append(np.array(img).flatten())
            train_label.append(int(label))
            
        for idx, (img, label) in enumerate(test_loader):
            test_data.append(np.array(img).flatten())
            test_label.append(int(label))
        train_data = np.array(train_data)
        train_label = np.array(train_label)
        test_data = np.array(test_data)
        test_label = np.array(test_label)
        print("Done! Shape of train data is: ", train_data.shape)
    
    elif isinstance(model, ResNet18):
        train_loader = DataLoader(train_set, batch_size=8, num_workers=16, shuffle=True, drop_last=True)
        valid_loader = DataLoader(valid_set, batch_size=1, num_workers=16, shuffle=True, drop_last=True)
        test_loader = DataLoader(test_set, batch_size=1, num_workers=16, shuffle=False, drop_last=False)
        
        # 在这里调节超参数
        # lr_sgd = 1e-4
        lr_adam = 1e-4
        epochs = 30
        optimizer = torch.optim.Adam(model.parameters(), lr=lr_adam, weight_decay=1e-4)
        # optimizer = torch.optim.SGD(model.parameters(), lr=lr_sgd, momentum=0.9, nesterov=True, weight_decay=1e-4)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0, last_epoch=-1)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        io.cprint(f"Using: {device}")
        total = sum([param.nelement() for param in model.parameters()])
        io.cprint("Number of model parameters: %.2fM" % (total/1e6))
    
    
    # 训练及测试
    io.cprint("Start to train...") 
    if isinstance(model, SVC):
        model.fit(train_data, train_label)
        train_pred = model.predict(train_data)
        
        train_acc = (train_pred == train_label).mean()
        io.cprint(f"TRAIN_ACC:{train_acc: .4f}")
    
        test_pred = model.predict(test_data)
        test_acc = (test_pred == test_label).mean()
        io.cprint(f"TEST_ACC:{test_acc: .4f}")
        
        test_prob = model.predict_proba(test_data)[:, 1] # 预测为“1”类的可能性
        if eval:
            evaluate(io, test_label, test_prob, test_pred)
        
        return model

    elif isinstance(model, ResNet18):
        model = train(args, io, model, train_loader, valid_loader, test_loader, optimizer, scheduler, device, epochs)
        test_and_eval(args, io, model, test_loader, device, eval=eval)
        
        return model

def tsinghua_test(args, io):
    io.cprint("\nGet Model...")
    if args.model == 'svm':
        model = main(args, io, eval=False)
    elif args.model == 'resnet':
        model = ResNet18(num_classes=2, use_dropout=False, use_init=False)
    print("Get Model Done!")
    
    io.cprint("\nLoading Tsinghua data...")
    dataset = load_Tsinghua_data('./stair/tsinghua/')
    data_transform = transforms.Compose([
        transforms.Resize([96, 96]),
        transforms.ToTensor(),
    ])
    test_set = StairDataset(dataset, 'test', data_transform)
    
    if isinstance(model, SVC):
        test_data = []
        test_label = []
        test_loader = DataLoader(test_set, batch_size=1, num_workers=16, shuffle=False, drop_last=False)
            
        for idx, (img, label) in enumerate(test_loader):
            test_data.append(np.array(img).flatten())
            test_label.append(int(label))
        test_data = np.array(test_data)
        test_label = np.array(test_label)
        print("Done! Shape of data is: ", test_data.shape)
        io.cprint("\nTest on Tsinghua dataset..")
        test_pred = model.predict(test_data)
        test_acc = (test_pred == test_label).mean()
        io.cprint(f"ACC:{test_acc: .4f}")
            
        test_prob = model.predict_proba(test_data)[:, 1] # 预测为“1”类的可能性
        evaluate(io, test_label, test_prob, test_pred)
    
    elif isinstance(model, ResNet18):
        test_loader = DataLoader(test_set, batch_size=1, num_workers=16, shuffle=False, drop_last=False)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        test_and_eval(args, io, model, test_loader, device, eval=True, best_path='./logs/exp_adam_3/models/model.t7')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Images classification')
    parser.add_argument('--exp_name', type=str, default='exp', help='Name of the experiment')
    parser.add_argument('--model', type=str, default='svm', choices=['svm', 'resnet'], help='Choose model')
    parser.add_argument('--dataset', type=str, default='public', choices=['public', 'tsinghua'], help='Choose dataset')
    args = parser.parse_args()
    
    
    _init_(args)
    
    io = IOStream('./logs/' + args.exp_name + '/run.log')
    io.cprint(str(args))
    
    if args.dataset == 'public':
        model = main(args, io)
    elif args.dataset == 'tsinghua':
        tsinghua_test(args, io)
    
